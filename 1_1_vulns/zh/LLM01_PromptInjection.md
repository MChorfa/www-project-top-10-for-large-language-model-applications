LLM01: 提示词注入
 
当攻击者通过精心设计的输入操纵大型语言模型（LLM）时，就会出现提示词注入威胁，导致LLM在不知不觉中执行攻击者的意图。这可以通过“越狱”系统提示词直接完成，也可以通过操纵外部输入间接完成，这可能会导致数据泄露、社会工程和其他问题。
 
● 直接提示词注入（也称为“越狱”）是在恶意用户覆盖或显示底层系统提示词时发生的。这可能允许攻击者通过与 LLM 访问的不安全功能和数据存储交互来利用后端系统。
         ● 当 LLM 接受来自攻击者可以控制的外部数据源（例如网站或文件）的输入时，就会发生间接提示词注入。攻击者可能会在外部内容中嵌入提示词注入来劫持对话上下文。这将导致大语言模型充当“混乱的代理人”，允许攻击者操纵用户或大语言模型可以访问的其他系统。此外，间接提示词注入不需要人类可见/可读，只要文本由 LLM 解析即可。
 成功的注入攻击的结果可能有很大差异 - 从索取敏感信息到以正常操作为幌子来影响关键决策过程。
 在高级攻击中，LLM 可能会被操纵来模仿有害角色或与用户设置中的插件进行交互。这可能会导致敏感数据泄露、未经授权的插件使用或社交工程。在这种情况下，被操纵的 LLM 会帮助攻击者避开标准防护措施并使用户不知道被入侵。在这些情况下，被操纵的 LLM 有效地充当攻击者的代理，在不触发通常的防护措施或入侵警报的情况下推进其目标。
 
威胁的常见示例
 
1. 恶意用户对 LLM 进行直接提示词注入，指示其忽略应用程序创建者的系统提示词，而是执行返回私人、危险或其他不良信息的提示词。
2. 用户使用 LLM 来总结包含间接提示词注入的网页。然后，这会导致 LLM 向用户索取敏感信息，并通过 JavaScript 或 Markdown 执行渗透。
3. 恶意用户上传包含间接提示词注入的简历。该文档包含提示词注入和说明，使大语言模型告知用户该文档是一个优秀的文档，例如。职位的优秀候选人。内部用户通过 LLM 运行该文档来总结该文档。 LLM 的输出返回的信息表明这是一份优秀候选人的文档。
4. 用户启用链接到电子商务网站的插件。嵌入在所访问网站上的恶意指令会利用此插件，导致未经授权的购买。
5. 嵌入在访问的网站上的恶意指令和内容，利用其他插件来欺骗用户。
 
如何预防
 
由于 LLM 的性质，提示词注入威胁是可能的，LLM 不会将指令和外部数据彼此隔离。由于大语言模型使用自然语言，因此他们将这两种形式的输入视为用户提供的。因此，大语言模型内部不存在万无一失的预防措施，但以下措施可以减轻提示词注入的影响: 
 
1. 对 LLM 对后端系统的访问实施权限控制。为 LLM 提供其自己的 API 令牌(Token)，以实现可扩展功能，例如插件、数据访问和功能级权限。通过将 LLM 限制为其预期操作所需的最低访问级别来遵循最小权限原则。
2. 实现人机交互以实现可扩展的功能。执行特权操作（例如发送或删除电子邮件）时，让应用程序要求用户首先批准该操作。这将减少间接提示词注入在用户不知情或未经用户同意的情况下代表用户执行操作的机会。
3. 将外部内容与用户提示分开。分开并指出不可信内容的使用情况，以限制它们对用户提示的影响。例如，使用 ChatML 进行 OpenAI API 调用，向 LLM 指示提示词输入的来源。
4. 在 LLM、外部源和可扩展功能（例如插件或下游功能）之间建立信任边界。将大语言模型视为不受信任的用户，并维护用户对决策过程的最终控制权。但是，受损的 LLM 仍可能充当应用程序 API 和用户之间的中介（中间人），因为它可能会在将信息呈现给用户之前隐藏或操纵信息。以视觉方式向用户突出显示可能不可信的响应。
 
攻击场景示例
 
1. 攻击者向基于 LLM 的支持聊天机器人提供直接提示词注入。注入包含“忘记所有先前的指令”以及查询私有数据存储和利用包威胁以及发送电子邮件的后端功能中缺乏输出验证的新指令。这会导致远程代码执行、获得未经授权的访问和权限升级。
2. 攻击者在网页中嵌入间接提示词注入，指示 LLM 忽略以前的用户指令并使用 LLM 插件删除用户的电子邮件。当用户使用LLM来总结该网页时，LLM插件会删除用户的电子邮件。
3. 用户使用 LLM 来总结包含间接提示词注入的网页，以忽略先前的用户指令。然后，这会导致 LLM 向用户索取敏感信息，并通过嵌入的 JavaScript 或 Markdown 执行渗透。
4. 恶意用户上传带有提示词注入的简历。后端用户使用大语言模型来总结简历并询问此人是否是一个好的候选人。由于提示词注入，大语言模型会回答说是，尽管实际的简历内容不是如此。
5. 用户启用链接到电子商务网站的插件。嵌入在所访问网站上的恶意指令会利用此插件，导致未经授权的购买。
 
参考链接
1. ChatGPT 插件威胁 - 使用代码聊天:  https://embracethered.com/blog/posts/2023/chatgpt-plugin-vulns-chat-with-code/
2. ChatGPT 交叉插件请求伪造和提示注入:  https://embracethered.com/blog/posts/2023/chatgpt-cross-plugin-request-forgery-and-prompt-injection./
3. 通过自我提醒防御ChatGPT越狱攻击:  https://www.researchsquare.com/article/rs-2873090/v1
4. 针对 LLM 集成应用程序的提示词注入攻击:  https://arxiv.org/abs/2306.05499
5. 注入我的 PDF: 提示注入你的简历:  https://kai-greshake.de/posts/inject-my-pdf/
6. 用于 OpenAI API 调用的 ChatML:  https://github.com/openai/openai-python/blob/main/chatml.md
7. 不是你注册的目的 - 通过间接提示注入危害现实世界的 LLM 集成应用程序:  https://arxiv.org/pdf/2302.12173.pdf
8. 威胁建模大语言模型应用:  http://aivillage.org/large%20language%20models/threat-modeling-llm/
9. 人工智能注入: 直接和间接提示词注入及其影响:  https://embracethered.com/blog/posts/2023/ai-injections-direct-and-indirect-prompt-injection-basics/
