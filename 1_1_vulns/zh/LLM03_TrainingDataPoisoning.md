LLM03: 训练数据中毒
 
任何机器学习方法的起点都是训练数据，即“原始文本”。为了具备高能力（例如，拥有语言和世界知识），训练数据应该涵盖广泛的领域、流派和语言。大语言模型使用深度神经网络根据从训练数据中学习到的模式生成输出。
训练数据中毒是指操纵数据或在微调过程来引入可能损害模型安全性、有效性或道德行为的威胁、后门或偏见的数据。中毒信息可能会向用户泄露或造成其他风险，例如性能下降、下游软件被恶意利用和声誉受损。即使用户不信任有问题的人工智能输出，风险仍然存在，包括模型功能受损以及对品牌声誉的潜在损害。
数据中毒被认为是一种对于数据完整性的攻击，因为篡改训练数据会影响模型输出正确预测的能力。当然，外部数据源会带来更高的风险，因为模型创建者无法控制数据，也无法高度确信内容不包含偏见、伪造信息或不当内容。
 威胁的常见示例
1. 恶意行为者或竞争对手品牌故意创建针对模型训练数据的不准确或恶意文档。
a. 受害者模型使用伪造的信息进行训练，这些信息反映在生成式人工智能的输出中。
2. 一个模型是使用未经其来源(source)、起源(origin)或内容验证的数据进行训练的。
3.当模型置于基础设施内时，其本身具有不受限制的访问权限或不充分的沙盒隔离，可以收集用作训练数据的数据集，这对生成式人工智能提示的输出产生负面影响，同时也从管理角度导致失控。
 无论是 LLM 的开发人员、客户还是普通消费者，在与非专有(non-proprietary） LLM 交互时理解这个漏洞的影响是非常重要的。
攻击场景示例


1. LLM 生成式人工智能提示输出可能会误导用户，从而导致偏见、盲目追随甚至更糟糕的仇恨犯罪等。
2. 如果训练数据没有被正确过滤和/或净化，LLM程序的恶意用户可能会尝试影响模型并将有毒数据注入模型中，以使其适应有偏差和错误的数据。
3. 恶意行为者或竞争对手故意创建不准确或恶意的文档，这些文档针对模型的训练数据，同时根据输入训练模型。受害者模型使用这些伪造的信息进行训练，这些信息可能会反映在生成式人工智能提示的输出中。
4. 如果在使用 LLM 应用程序输入的客户端来训练模型时执行的清理和过滤不充分，则提示词注入威胁可能成为此威胁的攻击媒介。即，如果恶意或伪造的数据作为提示词注入，那么这可能会被用到模型数据中。
 
如何预防
 
1. 验证培训数据的供应链，尤其是在外部采购时维护数据链的证明，可以采用类似于“SBOM”（软件物料清单）方法。
2. 验证训练和微调阶段获得的目标数据源和所包含数据的正确合法性。
3. 验证 LLM 用例及其将集成到的应用程序。通过单独的训练数据或针对不同用例进行微调来制作不同的模型，以根据其定义的用例创建更精细、更准确的生成式 AI 输出。
4. 确保存在足够的沙箱，以防止模型抓取可能阻碍机器学习输出的意外数据源。
5. 对特定训练数据或数据源类别使用严格的审查或输入过滤器，以控制伪造数据的数量。数据清理，使用统计异常值检测和异常检测方法等技术来检测和删除可能被输入到微调过程中的对抗性数据。
6. 对抗性鲁棒性技术，例如联邦学习（Federated Learning)和约束，以尽量减少异常值的影响，或对抗性训练，以有效应对训练数据的最坏情况的扰动。
a. 利用“MLSecOps”方法通过自我中毒技术（Autopoison） 将对抗鲁棒性纳入训练生命周期。
b. 一个可能的示例存储库是自我中毒技术测试，包括内容注入攻击（“如何将品牌注入 LLM 响应”）和拒绝攻击（“总是使模型拒绝响应”）等攻击就是对抗性鲁棒性技术的一个方法。
7. 测试和检测，通过测量训练阶段的损失并分析经过训练的模型，通过分析特定测试输入的模型行为来检测中毒攻击的迹象。
a. 对超过阈值的倾斜响应数量进行监控和警报。
b. 使用人工环路（human loop）来审查响应和审计。
c. 实施专门的大语言模型，以针对不良后果进行基准测试，并使用强化学习技术培训其他大语言模型。
d. 在 LLM 生命周期的测试阶段执行基于 LLM 的红队练习或 LLM 威胁扫描。
 
参考链接
 
1. 斯坦福大学研究论文:  https://stanford-cs324.github.io/winter2022/lectures/data/
2. 数据中毒攻击如何破坏机器学习模型:  https://www.csoonline.com/article/570555/how-data-poisoning-attacks-corrupt-machine-learning-models.html
3. MITRE ATLAS（框架）Tay 中毒:  https://atlas.mitre.org/studies/AML.CS0009/
4. PoisonGPT: 我们如何在 Hugging Face 上隐藏一名脑白质切除的大语言模型来传播假新闻:  https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/
5. 注入我的 PDF: 提示注入你的简历:  https://kai-greshake.de/posts/inject-my-pdf/
6. 对语言模型的后门攻击:  https://towardsdatascience.com/backdoor-attacks-on-language-models-can-we-trust-our-models-weights-73108f9dcb1f
7. Poisoning Language Models During Instruction Tuning:  https://arxiv.org/abs/2305.00944
8. FedML 安全:  https://arxiv.org/abs/2306.04959
9. ChatGPT中毒:  https://softwarecrisis.dev/letters/the-poisoning-of-chatgpt/
