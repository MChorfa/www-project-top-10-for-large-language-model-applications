LLM05: 供应链威胁
 
大语言模型的供应链可能很脆弱，会影响训练数据、机器学习模型和部署平台的完整性。这些威胁可能会导致有偏差的结果、安全威胁，甚至整个系统故障。传统上，威胁主要集中在软件组件上，但机器学习通过易受篡改和中毒攻击的第三方提供的预训练模型和训练数据扩展了这一点。
 最后，LLM 插件扩展可能会带来自己的威胁。这些在 LLM - 不安全插件设计中进行了描述，其中涵盖了 LLM 插件的编写并提供了对评估第三方插件有用的信息。
威胁的常见示例
 
1. 传统的第三方软件包威胁，包括过时或已弃用的组件。
2. 使用易受攻击的预训练模型进行微调。
3. 使用有毒的众包数据进行培训。
4. 使用不再维护的过时或已弃用的模型会导致安全问题。
5. 模型运营商的条款和条件以及数据隐私政策不明确，导致应用程序的敏感数据被用于模型训练以及后续的敏感信息暴露。这也可能适用于模型供应商使用受版权保护的材料带来的风险。
 
如何预防
 
1. 仔细审查数据源和供应商，包括条款和条件及其隐私政策，仅使用值得信赖的供应商。确保充分且经过独立审核的安全性到位，并且模型运营商策略与你的数据保护策略保持一致，即你的数据不会用于训练他们的模型；同样，寻求模型维护者的保证和法律缓解措施，防止使用受版权保护的材料。
2. 仅使用信誉良好的插件并确保它们已经过测试以满足你的应用程序要求。 LLM-不安全插件设计提供了有关不安全插件设计的LLM方面的信息，你应该对其进行测试，以减轻使用第三方插件的风险。
3. 了解并应用 OWASP Top 10 A06:2021 – 易受攻击和过时的组件中找到的缓解措施。这包括威胁扫描、管理和修补组件。对于能够访问敏感数据的开发环境，也应在这些环境中应用这些控制。
4. 使用软件物料清单 (SBOM) 维护最新的组件库存，以确保你拥有最新、准确且经过签名的库存，以防止篡改已部署的软件包。 SBOM 可用于快速检测新的零日期威胁并发出警报。
5. 在撰写本文时，SBOM 不涵盖模型、其工件和数据集；如果你的 LLM 申请使用自己的模型，你应该使用 MLOP 最佳实践和平台，提供具有数据、模型和实验跟踪功能的安全模型存储库。
6. 在使用外部模型和供应商时，还应该使用模型和代码签名。
7. 对所提供的模型和数据进行异常检测和对抗性稳健性测试可以帮助检测篡改和中毒，如训练数据中毒中所述；理想情况下，这应该是 MLOps 管道的一部分；然而，这些都是新兴技术，作为红队练习的一部分可能更容易实施。
8. 实施足够的监控以涵盖组件和环境威胁扫描、未经授权的插件的使用以及过时的组件（包括模型及其工件）。
9. 实施修补策略以减轻易受攻击或过时的组件的影响。确保应用程序依赖于 API 和底层模型的维护版本。
10. 定期审查和审核供应商的安全和访问，确保其安全状况或条款和条件不会发生变化。
 
攻击场景示例
 
1. 攻击者利用易受攻击的 Python 库来破坏系统。这发生在第一次开放人工智能数据泄露中。
2. 攻击者提供 LLM 插件来搜索航班，该插件会生成虚假链接，从而欺骗插件用户。
3. 攻击者利用 PyPi 包注册表诱骗模型开发人员下载受感染的包并在模型开发环境中窃取数据或升级权限。这是一次真实的攻击。
4. 攻击者毒害了一个专门从事经济分析和社会研究的公开预训练模型，以创建一个后门，生成错误信息和虚假新闻。他们将其部署在模型市场（例如 HuggingFace）上供受害者使用。
5. 攻击者会毒害公开可用的数据集，以帮助在微调模型时创建后门。后门巧妙地有利于不同市场的某些公司。
6. 供应商（外包开发商、托管公司等）的受感染员工窃取了数据、模型或代码窃取 IP。
7. LLM 运营商更改了其条款与条件和隐私政策，要求明确选择不使用应用程序数据进行模型训练，从而导致敏感数据的记忆。
 
参考链接
 
1. ChatGPT 数据泄露已确认，安全公司警告易受攻击的组件被利用:  https://www.securityweek.com/chatgpt-data-breach-confirmed-as-security-firm-warns-of-vulnerable-component-exploitation/
2. Open AI的Plugin审核流程:  https://platform.openai.com/docs/plugins/review
3. 受损的 PyTorch-nightly 依赖链:  https://pytorch.org/blog/compromished-nightly-dependency/
4. PoisonGPT: 我们如何在 Hugging Face 上隐藏一名脑白质切除的大语言模型来传播假新闻:  https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/
5. 美国陆军正在研究“AI BOM”的可能性:  https://defensescoop.com/2023/05/25/army-looking-at-the-possibility-of-ai-boms-bill-of-materials/
6. 机器学习中的故障模式:  https://learn.microsoft.com/en-us/security/engineering/failure-modes-in-machine-learning
7. 机器学习供应链妥协:  https://atlas.mitre.org/techniques/AML.T0010/
8. 机器学习的可迁移性: 从现象到使用对抗性样本的黑盒攻击:  https://arxiv.org/pdf/1605.07277.pdf
9. BadNets: 识别机器学习模型供应链中的威胁:  https://arxiv.org/abs/1708.06733
10. VirusTotal Poisoning:  https://atlas.mitre.org/studies/AML.CS0002/
