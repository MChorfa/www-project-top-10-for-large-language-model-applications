LLM06: 敏感信息披露
 
LLM 应用程序有可能通过其输出泄露敏感信息、专有算法或其他机密细节。这可能会导致未经授权访问敏感数据、知识产权、侵犯隐私和其他安全威胁。对于大语言模型申请的使用者来说，重要的是要了解如何安全地与大语言模型进行交互，并识别与无意中输入大语言模型可能在其他地方的输出中返回的敏感数据相关的风险。
 
为了减轻这种风险，LLM 应用程序应该执行足够的数据清理，以防止用户数据进入训练模型数据。 LLM 申请所有者还应制定适当的使用条款政策，以使消费者了解其数据的处理方式以及选择不将其数据包含在培训模型中的能力。
 
消费者-LLM应用程序交互形成双向信任边界，我们不能固有地信任客户端->LLM输入或LLM->客户端输出。值得注意的是，此威胁假设某些先决条件超出了范围，例如威胁建模练习、保护基础设施和足够的沙箱。在系统提示中添加关于 LLM 应返回的数据类型的限制可以在一定程度上缓解敏感信息泄露的情况，但 LLM 的不可预测性意味着此类限制可能并不总是得到遵守，并且可以通过提示注入或其他途径来规避.
 
威胁的常见示例
 
1. 大语言模型回复中的敏感信息过滤不完整或不正确。
2. 大语言模型培训过程中敏感数据的过度拟合或记忆。
3. 由于大语言模型的误解、缺乏数据清理方法或错误而导致机密信息意外泄露。
 
如何预防
 
1. 集成足够的数据清理和清理技术，以防止用户数据进入训练模型数据。
2. 实施强大的输入验证和清理方法来识别和过滤掉潜在的恶意输入，以防止模型中毒。
3. 当用数据丰富模型并微调（ https://github.com/OWASP/www-project-top-10-for-large-language-model-applications/wiki/Definitions ）模型时: （IE，在部署之前或期间输入模型的数据）
a. 微调数据中被视为敏感的任何内容都有可能泄露给用户。因此，应用最小权限规则，并且不要根据最高权限用户可以访问的信息来训练模型，而这些信息可能会显示给较低权限的用户。
b. 对外部数据源的访问（运行时数据的编排）应该受到限制。
c. 对外部数据源应用严格的访问控制方法，并采取严格的方法来维护安全的供应链。
 
攻击场景示例
 
1. 当以非恶意方式与 LLM 应用程序交互时，毫无戒心的合法用户 A 通过 LLM 暴露于某些其他用户数据。
2. 用户 A 的目标是一组精心设计的提示，以绕过 LLM 的输入过滤器和清理，从而导致其泄露有关应用程序其他用户的敏感信息 (PII)。
3. 由于用户自身或LLM申请者的疏忽，PII等个人数据通过训练数据泄露到模型中。这种情况可能会增加上述情况 1 或 2 的风险和概率。
 
参考链接
 
1. AI 数据泄露危机: 新工具可防止公司机密被提供给 ChatGPT:  https ://www.foxbusiness.com/politics/ai-data-leak-crisis-prevent-company-secrets-chatgpt
2. 从 ChatGPT 三星泄密事件中吸取的教训:  https://cybernews.com/security/chatgpt-samsung-leak-explained-lessons/
3. Cohere- 使用条款:  https://cohere.com/terms-of-use
4. AI Village-威胁建模示例:  https://aivillage.org/large%20language%20models/threat-modeling-llm/
5. OWASP AI 安全和隐私指南:  https://owasp.org/www-project-ai-security-and-privacy-guide/
