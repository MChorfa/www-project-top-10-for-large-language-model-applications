LLM09: 过度依赖(Overreliance)
 
当系统或人员在没有充分监督的情况下依赖大语言模型进行决策或内容生成时，就会发生过度依赖。虽然大语言模型可以产生富有创意和信息丰富的内容，但他们也可能产生事实上不正确、不适当或不安全的内容。这被称为幻觉或虚构，可能导致错误信息、沟通不畅、法律问题和声誉受损。
 
LLM 生成的源代码可能会引入未被注意到的安全威胁。这给应用程序的操作安全和保障带来了重大风险。这些风险表明严格审查流程的重要性，其中: 
● 监督
● 持续验证机制
● 风险免责声明
 
威胁的常见示例
 
1. LLM 提供不准确的信息作为回应，导致错误信息。
2. LLM 产生逻辑上不连贯或无意义的文本，虽然语法正确，但没有意义。
3. 大语言模型融合了不同来源的信息，创造了误导性内容。
4. LLM 建议不安全或有缺陷的代码，从而在使用到软件系统中时导致威胁。
5. 提供商未能向最终用户适当传达固有风险，从而导致潜在的有害后果。
 
如何预防
 
1. 定期监控和审查大语言模型的输出。使用自一致性（self-consistency）或投票技术来过滤掉不一致的文本。比较单个提示的多个模型响应可以更好地判断输出的质量和一致性。
2. 与可信的外部来源交叉检查大语言模型的输出。这一额外的验证层可以帮助确保模型提供的信息准确可靠。
3. 通过微调或嵌入( embeddings )来增强模型，以提高输出质量。与特定领域的调整模型相比，通用预训练模型更有可能产生不准确的信息。为此，可以采用快速工程、参数有效性调整 (parameter efficient tuning:PET)、完整模型调整和思维链（chain of thought）提示词等技术。
4. 实施自动验证机制，可以根据已知事实或数据交叉验证生成的输出。这可以提供额外的安全层并减轻与幻觉相关的风险。
5. 将复杂的任务分解为可管理的子任务并将其分配给不同的代理。这不仅有助于管理复杂性，而且还减少了产生幻觉的机会，因为每个代理都可以负责较小的任务。
6. 传达与使用大语言模型相关的风险和限制。这包括潜在的信息不准确和其他风险。有效的风险沟通可以让用户为潜在问题做好准备，并帮助他们做出明智的决策。
7. 构建 API 和用户界面，鼓励负责任和安全地使用大语言模型。这可能涉及内容过滤器、有关潜在错误的用户警告以及人工智能生成内容的清晰标签等措施。
8. 在开发环境中使用大语言模型时，建立安全编码实践和指南，以防止可能的威胁集成。
 
攻击场景示例
 
1. 一家新闻机构大量使用人工智能模型来生成新闻文章。恶意行为者利用这种过度依赖，向人工智能提供误导性信息，导致虚假信息的传播。人工智能无意中抄袭了内容，导致版权问题并降低了对组织的信任。
2. 软件开发团队利用 Codex 等人工智能系统来加快编码过程。由于不安全的默认设置或与安全编码实践不一致的建议，过度依赖人工智能的建议会给应用程序带来安全威胁。
3. 一家软件开发公司使用大语言模型来帮助开发人员。大语言模型建议使用不存在的代码库或软件包，而信任人工智能的开发人员在不知不觉中将恶意软件包集成到公司的软件中。这凸显了交叉检查人工智能建议的重要性，尤其是在涉及第三方代码或库时。
 
参考链接
 
1. 了解大语言模型幻觉:  https://towardsdatascience.com/llm-hallucinations-ec831dcd7786
2. 公司应该如何向用户传达大型语言模型的风险？:  https://techpolicy.press/how-should-companies-communicate-the-risks-of-large-language-models-to-users/
3. 一家新闻网站使用人工智能来撰写文章。这是一场新闻灾难:  https://www.washingtonpost.com/media/2023/01/17/cnet-ai-articles-journalism- Corrections/
4. AI 幻觉: 包裹风险:  https://vulcan.io/blog/ai-hallucinations-package-risk
5. 如何减少大型语言模型的幻觉:  https://thenewstack.io/how-to-reduce-the-hallucinations-from-large-language-models/
6. 减少幻觉的实用步骤:  https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination
